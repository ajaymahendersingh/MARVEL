{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "364156e7-f32b-4b30-8e05-f1d225ef7a56",
      "metadata": {
        "id": "364156e7-f32b-4b30-8e05-f1d225ef7a56"
      },
      "source": [
        "---\n",
        "# **High-Level Python ML Model to Synopsys ASIP Tools Comparable C**\n",
        "### **Authors:** Cian O'Mahoney, Pedro Kreutz Werle, Ajay Kumar M\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1266fede-24eb-491e-bdd2-9b13235c173b",
      "metadata": {
        "id": "1266fede-24eb-491e-bdd2-9b13235c173b"
      },
      "source": [
        "### Import the Required Packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "brGv4hklGQfk",
      "metadata": {
        "id": "brGv4hklGQfk"
      },
      "outputs": [],
      "source": [
        "!pip install apache-tvm==0.14.dev273\n",
        "!pip install tensorflow==2.12.0\n",
        "!pip install keras==2.12.0\n",
        "!pip install tflite==2.1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3944eab-0319-4837-a30e-e321da2576e7",
      "metadata": {
        "id": "f3944eab-0319-4837-a30e-e321da2576e7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pathlib\n",
        "import numpy as np\n",
        "import tarfile\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "import shutil\n",
        "import urllib.request\n",
        "\n",
        "import tvm\n",
        "import tvm.relay as relay\n",
        "from tvm.relay.backend import Executor, Runtime\n",
        "from tvm.micro import export_model_library_format\n",
        "\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb531ece-af9f-4463-9cd4-fda9b48920f1",
      "metadata": {
        "id": "eb531ece-af9f-4463-9cd4-fda9b48920f1"
      },
      "source": [
        "Folder name for all files generated by this notebook:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "FOLDER = \"./py2c_VGG16\""
      ],
      "metadata": {
        "id": "Dc1ATrUjEUL2"
      },
      "id": "Dc1ATrUjEUL2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ea16117b-4990-4955-9662-088f2b69a73c",
      "metadata": {
        "id": "ea16117b-4990-4955-9662-088f2b69a73c"
      },
      "source": [
        "### Download Images to Use as Input Data for Our Model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc619cc1-54a0-4bb5-aefa-fe5137ce8396",
      "metadata": {
        "id": "dc619cc1-54a0-4bb5-aefa-fe5137ce8396",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Download datasets\n",
        "os.makedirs(f\"{FOLDER}/downloads\")\n",
        "os.makedirs(f\"{FOLDER}/images\")\n",
        "\n",
        "urllib.request.urlretrieve(\n",
        "    \"https://data.deepai.org/stanfordcars.zip\", f\"{FOLDER}/downloads/target.zip\"\n",
        ")\n",
        "urllib.request.urlretrieve(\n",
        "    \"http://images.cocodataset.org/zips/val2017.zip\", f\"{FOLDER}/downloads/random.zip\"\n",
        ")\n",
        "\n",
        "# Extract them and rename their folders\n",
        "shutil.unpack_archive(f\"{FOLDER}/downloads/target.zip\", f\"{FOLDER}/downloads\")\n",
        "shutil.unpack_archive(f\"{FOLDER}/downloads/random.zip\", f\"{FOLDER}/downloads\")\n",
        "shutil.move(f\"{FOLDER}/downloads/cars_train/cars_train\", f\"{FOLDER}/images/target\")\n",
        "shutil.move(f\"{FOLDER}/downloads/val2017\", f\"{FOLDER}/images/random\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6eb496c1-9b8b-4b76-827a-1b05e5b16a7e",
      "metadata": {
        "id": "6eb496c1-9b8b-4b76-827a-1b05e5b16a7e"
      },
      "source": [
        "### Rescale the Images to 64x64 and Normalise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c768ca17-c349-48f7-af16-eccba6d855ec",
      "metadata": {
        "id": "c768ca17-c349-48f7-af16-eccba6d855ec"
      },
      "outputs": [],
      "source": [
        "IMAGE_SIZE = (64, 64, 3)\n",
        "unscaled_dataset = tf.keras.utils.image_dataset_from_directory(\n",
        "    f\"{FOLDER}/images\",\n",
        "    batch_size=32,\n",
        "    shuffle=True,\n",
        "    label_mode=\"categorical\",\n",
        "    image_size=IMAGE_SIZE[0:2],\n",
        ")\n",
        "rescale = tf.keras.layers.Rescaling(scale=1.0 / 255)\n",
        "full_dataset = unscaled_dataset.map(lambda im, lbl: (rescale(im), lbl))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba329e2e-f8ad-45b0-88e8-c478894b1732",
      "metadata": {
        "id": "ba329e2e-f8ad-45b0-88e8-c478894b1732"
      },
      "source": [
        "Display some sample images:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07653153-4200-4ab5-ace0-e13e64395757",
      "metadata": {
        "id": "07653153-4200-4ab5-ace0-e13e64395757"
      },
      "outputs": [],
      "source": [
        "num_target_class = len(os.listdir(f\"{FOLDER}/images/target/\"))\n",
        "num_random_class = len(os.listdir(f\"{FOLDER}/images/random/\"))\n",
        "print(f\"{FOLDER}/images/target contains {num_target_class} images\")\n",
        "print(f\"{FOLDER}/images/random contains {num_random_class} images\")\n",
        "\n",
        "# Show some samples and their labels\n",
        "SAMPLES_TO_SHOW = 10\n",
        "plt.figure(figsize=(20, 10))\n",
        "for i, (image, label) in enumerate(unscaled_dataset.unbatch()):\n",
        "    if i >= SAMPLES_TO_SHOW:\n",
        "        break\n",
        "    ax = plt.subplot(1, SAMPLES_TO_SHOW, i + 1)\n",
        "    plt.imshow(image.numpy().astype(\"uint8\"))\n",
        "    plt.title(list(label.numpy()))\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "577228a2-21b6-4c66-9610-97e8d4740fcd",
      "metadata": {
        "id": "577228a2-21b6-4c66-9610-97e8d4740fcd"
      },
      "source": [
        "### Split the Dataset into Training and Testing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c268a5c5-1b22-4754-abf1-deada2d6237d",
      "metadata": {
        "id": "c268a5c5-1b22-4754-abf1-deada2d6237d"
      },
      "outputs": [],
      "source": [
        "num_batches = len(full_dataset)\n",
        "train_dataset = full_dataset.take(int(num_batches * 0.8))\n",
        "validation_dataset = full_dataset.skip(len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a403831-b02c-4431-afb4-2489bb25953e",
      "metadata": {
        "id": "9a403831-b02c-4431-afb4-2489bb25953e"
      },
      "source": [
        "### Download Pretrained Model Weights:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained = tf.keras.applications.VGG16(\n",
        "    input_shape=IMAGE_SIZE, weights='imagenet', include_top=False\n",
        ")"
      ],
      "metadata": {
        "id": "TQP8ayzXLR9s"
      },
      "id": "TQP8ayzXLR9s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "1e90a9ba-09b1-4367-8ecc-a8d6c99da327",
      "metadata": {
        "id": "1e90a9ba-09b1-4367-8ecc-a8d6c99da327"
      },
      "source": [
        "### Modify Network to Fit Task:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D5VAup65YLMw",
      "metadata": {
        "id": "D5VAup65YLMw"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "model.add(tf.keras.layers.InputLayer(input_shape=IMAGE_SIZE))\n",
        "model.add(tf.keras.Model(inputs=pretrained.inputs, outputs=pretrained.layers[-5].output))\n",
        "\n",
        "model.add(tf.keras.layers.Reshape((-1,)))\n",
        "model.add(tf.keras.layers.Dropout(0.1))\n",
        "model.add(tf.keras.layers.Flatten())\n",
        "model.add(tf.keras.layers.Dense(2, activation=\"softmax\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d28a95b4-d550-428d-b3c1-981a37fc4919",
      "metadata": {
        "id": "d28a95b4-d550-428d-b3c1-981a37fc4919"
      },
      "source": [
        "### Train New Model on Training Data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bca282f9-8c15-42b8-aafa-22bec982b783",
      "metadata": {
        "id": "bca282f9-8c15-42b8-aafa-22bec982b783"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0005),\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"],\n",
        ")\n",
        "model.fit(train_dataset, validation_data=validation_dataset, epochs=30, verbose=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e16fdd-5727-4219-95dd-0cf038adf873",
      "metadata": {
        "id": "85e16fdd-5727-4219-95dd-0cf038adf873"
      },
      "source": [
        "### Reduce Program Size by Quantising Model Parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2f898bc1-79b6-42a9-afc6-9c77ea0dde47",
      "metadata": {
        "id": "2f898bc1-79b6-42a9-afc6-9c77ea0dde47"
      },
      "outputs": [],
      "source": [
        "def representative_dataset():\n",
        "    for image_batch, label_batch in full_dataset.take(10):\n",
        "        yield [image_batch]\n",
        "\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "quantized_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97b35186-a176-439a-9096-c4f579d2bdd6",
      "metadata": {
        "id": "97b35186-a176-439a-9096-c4f579d2bdd6"
      },
      "source": [
        "Download the Tensorflow model at this point"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "directory_path = f\"{FOLDER}/models\"\n",
        "os.makedirs(directory_path, exist_ok=True)\n",
        "QUANTIZED_MODEL_PATH = f\"{FOLDER}/models/quantized.tflite\"\n",
        "with open(QUANTIZED_MODEL_PATH, \"wb\") as f:\n",
        "    f.write(quantized_model)"
      ],
      "metadata": {
        "id": "2Sh3LBFN75hO"
      },
      "id": "2Sh3LBFN75hO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "de322c7a-bc3f-4688-a0ec-017a9b08e722",
      "metadata": {
        "id": "de322c7a-bc3f-4688-a0ec-017a9b08e722"
      },
      "source": [
        "### Convert Tensorflow Model to C Source Code Using TVM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51777297-c069-4e8d-9224-445caa671e10",
      "metadata": {
        "id": "51777297-c069-4e8d-9224-445caa671e10"
      },
      "outputs": [],
      "source": [
        "# Method to load model is different in TFLite 1 vs 2\n",
        "try:  # TFLite 2.1 and above\n",
        "    import tflite\n",
        "    tflite_model = tflite.Model.GetRootAsModel(quantized_model, 0)\n",
        "except AttributeError:  # Fall back to TFLite 1.14 method\n",
        "    import tflite.Model\n",
        "    tflite_model = tflite.Model.Model.GetRootAsModel(quantized_model, 0)\n",
        "\n",
        "# Convert to the Relay intermediate representation\n",
        "mod, params = tvm.relay.frontend.from_tflite(tflite_model)\n",
        "\n",
        "# Set configuration flags to improve performance\n",
        "target = tvm.target.Target(\"c\")  # Specify target as generic C application.\n",
        "runtime = tvm.relay.backend.Runtime(\"crt\")  # Specify C runtime.\n",
        "executor =  tvm.relay.backend.Executor(\"aot\", {\"unpacked-api\": True, \"interface-api\": \"c\", \"workspace-byte-alignment\": 8})\n",
        "config = {\"tir.disable_vectorize\": True}\n",
        "\n",
        "# Convert to the MLF intermediate representation\n",
        "with tvm.transform.PassContext(opt_level=3, config=config):\n",
        "    mod = tvm.relay.build(mod, target, runtime=runtime, executor=executor, params=params)\n",
        "\n",
        "parameter_size = len(tvm.runtime.save_param_dict(mod.get_params()))\n",
        "print(f\"Model parameter size: {parameter_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "30f0f73a-455d-4e11-8548-b290e66868a2",
      "metadata": {
        "id": "30f0f73a-455d-4e11-8548-b290e66868a2"
      },
      "source": [
        "### Download Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4d88e7b-5443-4eba-a492-565412f59983",
      "metadata": {
        "id": "e4d88e7b-5443-4eba-a492-565412f59983"
      },
      "outputs": [],
      "source": [
        "BUILD_DIR = pathlib.Path(FOLDER)\n",
        "BUILD_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "# Now, we export the model into a tar file:\n",
        "MODEL_NAME = \"VGG16\"\n",
        "TAR_PATH = pathlib.Path(BUILD_DIR) / str(MODEL_NAME + \".tar\")\n",
        "export_model_library_format(mod, TAR_PATH)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.15"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
